{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Adaptive Importance Sampling algorithm with openturns\n",
    "\n",
    "Source of the algorithm : J. Morio & M. Balesdent, Estimation of Rare Event Probabilities in Complex Aerospace and Other Systems, A Practical Approach, Elsevier, 2015\n",
    "\n",
    "\n",
    "The theory is given for a failure event defined as $\\phi(\\mathbf{X})>S$ with $\\mathbf{X}$ a random vector following a joint PDF $h_0$, $S$ a threshold and $\\phi$ a limit state function, without loss of generality.\n",
    "\n",
    "The IS probability estimate by Importance Sampling $\\widehat{P}^{IS}$ is given by \n",
    "\\begin{equation}\n",
    "\\widehat{P}^{IS}=\\frac{1}{N} \\sum_{i=1}^{N} {\\bf 1}_{\\phi(\\mathbf{X}_i)>S} \\frac{h_0(\\mathbf{X}_i)}{h(\\mathbf{X}_i)}.\n",
    "\\label{ISeq}\n",
    "\\end{equation}\n",
    "\n",
    "It is well-known that the optimal density minimizing the variance of the estimator $h_{opt}$ is  defined as\n",
    "\\begin{equation}\n",
    "h_{opt}=\\frac{{\\mathbf 1}_{\\phi(x)>S}h_0}{P}.\n",
    "\\label{opt}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "with $P$ the failure probability and is inaccessible in practice since this probability is unknown. \n",
    "\n",
    "\n",
    "The objective of Non parametric Adaptive Importance Sampling (NAIS) technique is to approximate the IS optimal auxiliary density given in the preceding equation  with a kernel function (e.g. Gaussian kernel). NAIS does not require the choice of a parametric pdf family  as Cross Entropy and is thus more flexible than a parametric model. Its iterative principle is relatively similar to CE optimization and can be described by the following steps.\n",
    "\n",
    "\n",
    "1. $k=1$ and set $\\rho \\in [0,1]$\n",
    "2. Generate the population $\\mathbf{X}_1^{(k)},\\dots,\\mathbf{X}_N^{(k)}$ according to the pdf $h_{k-1}$, apply the function $\\phi$ in order to have $Y_1^{(k)}=\\phi(\\mathbf{X}_1^{(k)}),\\ldots,Y_N^{(k)}=\\phi(\\mathbf{X}_N^{(k)})$\n",
    "3. Compute the empirical $\\rho$-quantile $q_k=\\min(S, Y^{(k)}_{\\left\\lfloor\\rho N\\right\\rfloor})$\n",
    "4. Estimate $I_k= \\frac{1}{kN} \\displaystyle \\sum_{j=1}^{k}\\sum_{i=1}^{N} {\\bf 1}_{\\phi(\\mathbf{X}_i^{(j)}) \\geq q_k} \\frac{h_0(\\mathbf{X}_i^{(j)})}{h_{j-1}(\\mathbf{X}_i^{(j)})}$ \n",
    "5. Update the Gaussian kernel sampling pdf with\n",
    "\\begin{equation}\n",
    "h_{k}(\\mathbf{X})=\\frac{1}{k N I_k \\det\\left(B_{k+1}\\right)}\\sum_{j=1}^{k}\\sum_{i=1}^{N}  w_{j}(\\mathbf{X}_i^{(j)})K_d\\left(B_{k+1}^{-1}\\left(\\mathbf{X}-\\mathbf{X}_i^{(j)}\\right)\\right),\n",
    "\\end{equation}\n",
    "where $K_d$ is the standard $d$-dimensional Gaussian function with zero mean and a diagonal covariance matrix $B_{k+1}=diag(b^1_{k+1},...,b^d_{k+1})$ and $w_j={\\bf 1}_{\\phi(\\mathbf{X}_i^{(j)}) \\geq q_k} \\frac{h_0(\\mathbf{X}_i^{(j)})}{h_{j-1}(\\mathbf{X}_i^{(j)})}$. The coefficients of matrix $B_{k+1}$ can be approximated (Silverman Rule) or postulated according to the AMISE (Asymptotic Mean Integrated Square Error) criterion for example. \n",
    "6. If $q_k<S$, $k\\leftarrow k+1$, go to Step 2\n",
    "7. Estimate the probability $\\widehat{P}^{NAIS}(\\phi(\\mathbf{\\mathbf{X}}>S))=\\frac{1}{N}\\displaystyle \\sum_{i=1}^{N} 1_{\\phi(\\mathbf{X}_i^{(k)})>S} \\frac{h_0(\\mathbf{X}_i^{(k)})}{h_{k-1}(\\mathbf{X}_i^{(k)})}$\n",
    "\n",
    "\n",
    "The NAIS algorithm with the Silverman rule is coded in the following class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openturns as ot\n",
    "from NAISAlgorithm import NAISAlgorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments\n",
    "\n",
    "http://openturns.github.io/openturns/master/examples/reliability_sensitivity/estimate_probability_monte_carlo.html\n",
    "\n",
    "\n",
    "We consider a simple beam stressed by a traction load F at both sides.\n",
    "\n",
    "The geometry is supposed to be deterministic; the diameter D is equal to:\n",
    "\n",
    "$D=0.02 \\textrm{ (m)}.$\n",
    "\n",
    "By definition, the yield stress is the load divided by the surface. Since the surface is \\pi D^2/4, the stress is:\n",
    "\n",
    "$S = \\frac{F}{\\pi D^2/4}.$\n",
    "\n",
    "Failure occurs when the beam plastifies, i.e. when the axial stress gets larger than the yield stress:\n",
    "\n",
    "$R - \\frac{F}{\\pi D^2/4} \\leq 0$\n",
    "\n",
    "where R is the strength.\n",
    "\n",
    "Therefore, the limit state function G is:\n",
    "\n",
    "$G(R,F) = R - \\frac{F}{\\pi D^2/4},$\n",
    "\n",
    "for any R,F\\in\\mathbb{R}.\n",
    "\n",
    "The value of the parameter D is such that:\n",
    "\n",
    "$D^2/4 = 10^{-4},$\n",
    "\n",
    "which leads to the equation:\n",
    "\n",
    "$G(R,F) = R - \\frac{F}{10^{-4} \\pi}.$\n",
    "\n",
    "with\n",
    "\n",
    "$R \\sim LogNormal(\\mu_R=3\\times 10^6, \\sigma_R=3\\times 10^5) [Pa]$\n",
    "\n",
    "$F \\sim Normal(\\mu_F=750, \\sigma_F=50) [N]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the event\n",
    "distribution_R = ot.LogNormalMuSigma(300.0, 30.0, 0.0).getDistribution()\n",
    "distribution_F = ot.Normal(75e3, 5e3)\n",
    "marginals = [distribution_R, distribution_F]\n",
    "distribution = ot.ComposedDistribution(marginals)\n",
    "\n",
    "# create the model\n",
    "model = ot.SymbolicFunction(['R', 'F'], ['R-F/(pi_*100.0)'])\n",
    "\n",
    "#create the event \n",
    "vect = ot.RandomVector(distribution)\n",
    "G = ot.CompositeRandomVector(model, vect)\n",
    "event = ot.ThresholdEvent(G, ot.Less(), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination of reference probability\n",
    "#MonteCarlo experiment\n",
    "n_MC = 1e6\n",
    "\n",
    "# create a Monte Carlo algorithm\n",
    "experiment = ot.MonteCarloExperiment()\n",
    "algo = ot.ProbabilitySimulationAlgorithm(event, experiment)\n",
    "algo.setMaximumOuterSampling(int(n_MC))\n",
    "algo.setMaximumCoefficientOfVariation(0.01)\n",
    "algo.run()\n",
    "# retrieve results\n",
    "result = algo.getResult()\n",
    "probability = result.getProbabilityEstimate()\n",
    "print('Pf=', probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the algorithm\n",
    "n_IS= 2500 # Number of samples at each iteration\n",
    "rho_quantile= 25 # Quantile determining the percentage of failure samples in the current population \n",
    "\n",
    "# Definition of the algoritm\n",
    "NAIS = NAISAlgorithm(event,n_IS,rho_quantile)\n",
    "\n",
    "# Run of the algorithm\n",
    "NAIS.compute_proba()\n",
    "print('Probability of failure:',NAIS.getFailureProbability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
