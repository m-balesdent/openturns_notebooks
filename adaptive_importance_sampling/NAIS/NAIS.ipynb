{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Adaptive Importance Sampling algorithm with openturns\n",
    "\n",
    "Source of the algorithm : J. Morio & M. Balesdent, Estimation of Rare Event Probabilities in Complex Aerospace and Other Systems, A Practical Approach, Elsevier, 2015\n",
    "\n",
    "\n",
    "The theory is given for a failure event defined as $\\phi(\\mathbf{X})>S$ with $\\mathbf{X}$ a random vector following a joint PDF $h_0$, $S$ a threshold and $\\phi$ a limit state function, without loss of generality.\n",
    "\n",
    "The IS probability estimate by Importance Sampling $\\widehat{P}^{IS}$ is given by \n",
    "\\begin{equation}\n",
    "\\widehat{P}^{IS}=\\frac{1}{N} \\sum_{i=1}^{N} {\\bf 1}_{\\phi(\\mathbf{X}_i)>S} \\frac{h_0(\\mathbf{X}_i)}{h(\\mathbf{X}_i)}.\n",
    "\\label{ISeq}\n",
    "\\end{equation}\n",
    "\n",
    "It is well-known that the optimal density minimizing the variance of the estimator $h_{opt}$ is  defined as\n",
    "\\begin{equation}\n",
    "h_{opt}=\\frac{{\\mathbf 1}_{\\phi(x)>S}h_0}{P}.\n",
    "\\label{opt}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "with $P$ the failure probability and is inaccessible in practice since this probability is unknown. \n",
    "\n",
    "\n",
    "The objective of Non parametric Adaptive Importance Sampling (NAIS) technique is to approximate the IS optimal auxiliary density given in the preceding equation  with a kernel function (e.g. Gaussian kernel). NAIS does not require the choice of a parametric pdf family  as Cross Entropy and is thus more flexible than a parametric model. Its iterative principle is relatively similar to CE optimization and can be described by the following steps.\n",
    "\n",
    "\n",
    "1. $k=1$ and set $\\rho \\in [0,1]$\n",
    "2. Generate the population $\\mathbf{X}_1^{(k)},\\dots,\\mathbf{X}_N^{(k)}$ according to the pdf $h_{k-1}$, apply the function $\\phi$ in order to have $Y_1^{(k)}=\\phi(\\mathbf{X}_1^{(k)}),\\ldots,Y_N^{(k)}=\\phi(\\mathbf{X}_N^{(k)})$\n",
    "3. Compute the empirical $\\rho$-quantile $q_k=\\min(S, Y^{(k)}_{\\left\\lfloor\\rho N\\right\\rfloor})$\n",
    "4. Estimate $I_k= \\frac{1}{kN} \\displaystyle \\sum_{j=1}^{k}\\sum_{i=1}^{N} {\\bf 1}_{\\phi(\\mathbf{X}_i^{(j)}) \\geq q_k} \\frac{h_0(\\mathbf{X}_i^{(j)})}{h_{j-1}(\\mathbf{X}_i^{(j)})}$ \n",
    "5. Update the Gaussian kernel sampling pdf with\n",
    "\\begin{equation}\n",
    "h_{k}(\\mathbf{X})=\\frac{1}{k N I_k \\det\\left(B_{k+1}\\right)}\\sum_{j=1}^{k}\\sum_{i=1}^{N}  w_{j}(\\mathbf{X}_i^{(j)})K_d\\left(B_{k+1}^{-1}\\left(\\mathbf{X}-\\mathbf{X}_i^{(j)}\\right)\\right),\n",
    "\\end{equation}\n",
    "where $K_d$ is the standard $d$-dimensional Gaussian function with zero mean and a diagonal covariance matrix $B_{k+1}=diag(b^1_{k+1},...,b^d_{k+1})$ and $w_j={\\bf 1}_{\\phi(\\mathbf{X}_i^{(j)}) \\geq q_k} \\frac{h_0(\\mathbf{X}_i^{(j)})}{h_{j-1}(\\mathbf{X}_i^{(j)})}$. The coefficients of matrix $B_{k+1}$ can be approximated (Silverman Rule) or postulated according to the AMISE (Asymptotic Mean Integrated Square Error) criterion for example. \n",
    "6. If $q_k<S$, $k\\leftarrow k+1$, go to Step 2\n",
    "7. Estimate the probability $\\widehat{P}^{NAIS}(\\phi(\\mathbf{\\mathbf{X}}>S))=\\frac{1}{N}\\displaystyle \\sum_{i=1}^{N} 1_{\\phi(\\mathbf{X}_i^{(k)})>S} \\frac{h_0(\\mathbf{X}_i^{(k)})}{h_{k-1}(\\mathbf{X}_i^{(k)})}$\n",
    "\n",
    "\n",
    "The NAIS algorithm with the Silverman rule is coded in the following class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openturns as ot\n",
    "from NAISAlgorithm import NAISAlgorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical experiments : Four branch function\n",
    "\n",
    "$$G(x_1,x_2) = min \\begin{pmatrix}3+0.1(x_1-x_2)^2-\\frac{(x_1+x_2)}{\\sqrt{2}};\\\\3+0.1(x_1-x_2)^2+\\frac{(x_1+x_2)}{\\sqrt{2}};\\\\\n",
    "(x_1-x_2)+ \\frac{k}{\\sqrt{2}};\\\\\n",
    "(x_2-x_1)+ \\frac{k}{\\sqrt{2}}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "with : \n",
    "* $k$ is equal to 6 or 7\n",
    "* $x_1 \\sim \\mathcal{N}(0,1)$\n",
    "* $x_2 \\sim \\mathcal{N}(0,1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of limit state function\n",
    "def four_branch(x):\n",
    "    x1 = x[0]\n",
    "    x2  = x[1]\n",
    "    k = x[2]\n",
    "    \n",
    "    g1 = 3+0.1*(x1-x2)**2-(x1+x2)/np.sqrt(2)\n",
    "    g2 = 3+0.1*(x1-x2)**2+(x1+x2)/np.sqrt(2)\n",
    "    g3 = (x1-x2)+k/np.sqrt(2)\n",
    "    g4 =(x2-x1)+k/np.sqrt(2)\n",
    "    \n",
    "    return [min((g1,g2,g3,g4))]\n",
    "\n",
    "\n",
    "# Definition de la pythonfunction generale\n",
    "my_four_branch = ot.PythonFunction(3, 1, four_branch)\n",
    "\n",
    "# Transformation de la pythonfunction vers parametricfunction en figeant le parametre k \n",
    "index_frozen = [2]\n",
    "my_four_branch_6 = ot.ParametricFunction(my_four_branch, index_frozen, [6])\n",
    "my_four_branch_7 = ot.ParametricFunction(my_four_branch, index_frozen, [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of input variable PDF\n",
    "\n",
    "dim_inputs = 2\n",
    "dist_x = ot.Normal([0.0, 0.0], [1.0, 1.0], ot.CorrelationMatrix(dim_inputs))\n",
    "inputVector = ot.RandomVector(dist_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination of reference probability\n",
    "#MonteCarlo experiment\n",
    "\n",
    "n_MC = np.int(1e6)\n",
    "\n",
    "# Creation of event\n",
    "ot.RandomGenerator.SetSeed(1)\n",
    "\n",
    "vect = ot.RandomVector(dist_x)\n",
    "G = ot.CompositeRandomVector(my_four_branch_7, vect)\n",
    "event = ot.ThresholdEvent(G, ot.Less(), 0.0)\n",
    "\n",
    "# create a Monte Carlo algorithm\n",
    "experiment = ot.MonteCarloExperiment()\n",
    "algo = ot.ProbabilitySimulationAlgorithm(event, experiment)\n",
    "algo.setMaximumOuterSampling(int(n_MC))\n",
    "algo.run()\n",
    "\n",
    "# retrieve results\n",
    "result = algo.getResult()\n",
    "probability = result.getProbabilityEstimate()\n",
    "print('Pf=', probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the algorithm\n",
    "n_IS= 1000 # Number of samples at each iteration\n",
    "rho_quantile= 25 # Quantile determining the percentage of failure samples in the current population \n",
    "\n",
    "# Definition of the algoritm\n",
    "NAIS = NAISAlgorithm(event,n_IS,rho_quantile)\n",
    "\n",
    "# Run of the algorithm\n",
    "NAIS.run()\n",
    "NAIS_result = NAIS.getResult()\n",
    "\n",
    "print('Probability of failure:',NAIS_result.getProbabilityEstimate())\n",
    "print('Samples:',NAIS_result.getSamples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot \n",
    "import numpy as np\n",
    "#Plot of surrogate model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "grid_size = 100\n",
    "x1 = np.linspace(-4,4,grid_size)\n",
    "x2 = np.linspace(-4,4,grid_size)\n",
    "\n",
    "xx1,xx2 = np.meshgrid(x1,x2)\n",
    "\n",
    "xx1_ = xx1.reshape((grid_size**2,1))\n",
    "xx2_ = xx2.reshape((grid_size**2,1))\n",
    "\n",
    "x = np.concatenate((xx1_,xx2_),1)\n",
    "                  \n",
    "y_true = np.array(my_four_branch_7(x))\n",
    "\n",
    "y_true = y_true.reshape((grid_size,grid_size))\n",
    "\n",
    "cmap = mpl.cm.hsv\n",
    "norm = mpl.colors.Normalize(vmin=-10, vmax=10)\n",
    "%matplotlib inline \n",
    "fig, (ax0) = plt.subplots(ncols=1,figsize=(9,9))\n",
    "im1 = ax0.pcolormesh(xx1,xx2,y_true,norm = norm,cmap = cmap)\n",
    "fig.colorbar(im1, ax=ax0)\n",
    "ax0.title.set_text('True function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of auxiliary density \n",
    "\n",
    "density_4_branch = NAIS_result.getAuxiliaryDensity()\n",
    "density_4_branch.drawPDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of samples on function \n",
    "\n",
    "import numpy as np\n",
    "#Plot of surrogate model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "grid_size = 100\n",
    "x1 = np.linspace(-4,4,grid_size)\n",
    "x2 = np.linspace(-4,4,grid_size)\n",
    "\n",
    "xx1,xx2 = np.meshgrid(x1,x2)\n",
    "\n",
    "xx1_ = xx1.reshape((grid_size**2,1))\n",
    "xx2_ = xx2.reshape((grid_size**2,1))\n",
    "\n",
    "x = np.concatenate((xx1_,xx2_),1)\n",
    "                  \n",
    "y_true = np.array(my_four_branch_7(x))\n",
    "\n",
    "y_true = y_true.reshape((grid_size,grid_size))\n",
    "\n",
    "cmap = mpl.cm.hsv\n",
    "norm = mpl.colors.Normalize(vmin=-10, vmax=10)\n",
    "%matplotlib inline \n",
    "fig, (ax0) = plt.subplots(ncols=1,figsize=(9,9))\n",
    "im1 = ax0.pcolormesh(xx1,xx2,y_true,norm = norm,cmap = cmap)\n",
    "fig.colorbar(im1, ax=ax0)\n",
    "ax0.title.set_text('True function')\n",
    "\n",
    "Samples = NAIS_result.getSamples()\n",
    "plt.scatter(Samples[:,0],Samples[:,1],linewidths = 0.01,marker='+',color='r')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-4,4])\n",
    "ax.set_ylim([-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
